{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§¹ Laptop Features Cleaning & Preprocessing\n",
                "\n",
                "## ðŸŽ¯ Objective\n",
                "This notebook focuses on cleaning and standardizing the critical features for laptop price prediction:\n",
                "- **RAM**: `RAM_SIZE` (Standardizing to GB), `RAM_TYPE` (Handling missings).\n",
                "- **Storage**: `SSD_SIZE`, `HDD_SIZE`, `STORAGE_SIZE` (Parsing fuzzy strings like 'GO'/'TO', resolving missings).\n",
                "- **Feature Engineering**: Creating a clean `STORAGE_TYPE` feature.\n",
                "\n",
                "We will handle French units (`1 TO`, `500 GO`), impute missing values using logic (\"Rescue Mission\"), and drop only the rows that are completely unusable (~10%)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import re\n",
                "\n",
                "# Setting visualization style\n",
                "sns.set_style(\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (10, 6)\n",
                "\n",
                "# Define file paths\n",
                "FILE_PATH = 'full_merged_dataset_copy.csv'\n",
                "OUTPUT_PATH = 'cleaned_dataset_leena.csv'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data\n",
                "We start by loading the dataset. We'll use the 'NedToBeFilled' placeholder as NaN to make our life easier."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = pd.read_csv(FILE_PATH)\n",
                "\n",
                "# Replace 'NeedToBeFilled' with actual NaN values for easier analysis\n",
                "df.replace('NeedToBeFilled', np.nan, inplace=True)\n",
                "\n",
                "print(f\"Initial Dataset Shape: {df.shape}\")\n",
                "# Display some rows with mixed units to verify the problem\n",
                "print(df[['RAM_SIZE', 'SSD_SIZE', 'HDD_SIZE', 'STORAGE_SIZE']].sample(5))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Helper Function: The 'Standardizer'\n",
                "\n",
                "### Why this step?\n",
                "Our data contains messy strings like `500 GO`, `1 TO`, `512 MB`. Computers can't do math on text. \n",
                "We need a function that:\n",
                "1. Finds the number.\n",
                "2. Identifies the unit (`TB`/`TO` -> *1024, `MB` -> /1024).\n",
                "3. Returns a clean float in **GB**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def parse_size_to_gb(value):\n",
                "    if pd.isna(value) or str(value).strip().lower() in ['nan', 'none', '']:\n",
                "        return np.nan\n",
                "    \n",
                "    text = str(value).upper().strip()\n",
                "    \n",
                "    # Standardize French units\n",
                "    text = text.replace('GO', 'GB').replace('TO', 'TB').replace('MO', 'MB')\n",
                "    \n",
                "    # Regex to find the first occurring number (int or float)\n",
                "    match = re.search(r'(\\d+(?:\\.\\d+)?)', text)\n",
                "    if not match:\n",
                "        return np.nan\n",
                "        \n",
                "    number = float(match.group(1))\n",
                "    \n",
                "    # Logic for unit conversion\n",
                "    if 'TB' in text:\n",
                "        return number * 1024\n",
                "    elif 'MB' in text:\n",
                "        return number / 1024\n",
                "    else:\n",
                "        # Default to GB if no unit specified or 'GB' is found\n",
                "        return number\n",
                "\n",
                "# Test the function on edge cases\n",
                "test_values = ['16 GB', '512 MO', '1 TO', '500GO', 'NeedToBeFilled']\n",
                "print(\"Testing Parser:\")\n",
                "for v in test_values:\n",
                "    print(f\"'{v}' -> {parse_size_to_gb(v)} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Cleaning RAM_SIZE\n",
                "\n",
                "### What are we doing?\n",
                "We apply the standardizer to `RAM_SIZE`. \n",
                "Then, for the ~7.6% of missing values, we impute using the **Median**. We choose Median over Mean to avoid skew from ultra-high-end servers (e.g., 128GB RAM)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Clean\n",
                "df['RAM_GB'] = df['RAM_SIZE'].apply(parse_size_to_gb)\n",
                "\n",
                "# 2. Visualize before imputation\n",
                "plt.figure(figsize=(10,4))\n",
                "sns.histplot(df['RAM_GB'].dropna(), bins=20, kde=False, color='skyblue')\n",
                "plt.title('Distribution of RAM (GB) Before Imputation')\n",
                "plt.xlabel('GB')\n",
                "plt.show()\n",
                "\n",
                "# 3. Impute\n",
                "median_ram = df['RAM_GB'].median()\n",
                "missing_ram_count = df['RAM_GB'].isna().sum()\n",
                "print(f\"Missing RAM rows: {missing_ram_count}. Imputing with median: {median_ram} GB\")\n",
                "\n",
                "df['RAM_GB'].fillna(median_ram, inplace=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Handling RAM_TYPE\n",
                "\n",
                "### Decision\n",
                "We found that **80%** of `RAM_TYPE` values are missing. \n",
                "Any attempt to guess these (e.g., by checking CPU gen) is complex and risky. A wrong guess biases the model.\n",
                "**Verdict**: Fill with 'Unknown'."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['RAM_TYPE_CLEAN'] = df['RAM_TYPE'].fillna('Unknown').str.upper().str.strip()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Storage Rescue Mission ðŸš‘\n",
                "\n",
                "### The Problem\n",
                "- `SSD_SIZE` missing: ~46%\n",
                "- `HDD_SIZE` missing: ~96%\n",
                "- **Opportunity**: `STORAGE_SIZE` column has textual info for 17,000+ rows where specific columns are empty!\n",
                "\n",
                "### The Strategy\n",
                "1. **Clean** existing `SSD` and `HDD` columns to GB.\n",
                "2. **Rescue**: If both are NaN, parse `STORAGE_SIZE`. If it says \"HDD\", assume HDD. Else, assume SSD (Safest bet for modern laptops).\n",
                "3. **Zero-fill**: If a row has SSD but missing HDD, assume HDD is 0 (and vice-versa)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Parse existing columns\n",
                "df['SSD_GB'] = df['SSD_SIZE'].apply(parse_size_to_gb)\n",
                "df['HDD_GB'] = df['HDD_SIZE'].apply(parse_size_to_gb)\n",
                "\n",
                "# Step 2: Rescue Logic function\n",
                "def rescue_storage(row):\n",
                "    ssd = row['SSD_GB']\n",
                "    hdd = row['HDD_GB']\n",
                "    storage_text = str(row['STORAGE_SIZE']).upper()\n",
                "    \n",
                "    # Only act if both specific columns are missing\n",
                "    if pd.isna(ssd) and pd.isna(hdd):\n",
                "        # Try to parse the fallback text\n",
                "        val = parse_size_to_gb(storage_text)\n",
                "        if pd.notna(val):\n",
                "            if 'HDD' in storage_text or 'HARD DISK' in storage_text:\n",
                "                return pd.Series([np.nan, val]) # SSD is NaN, HDD is Found Val\n",
                "            else:\n",
                "                return pd.Series([val, np.nan]) # SSD is Found Val (Default), HDD NaN\n",
                "                \n",
                "    return pd.Series([ssd, hdd])\n",
                "\n",
                "print(\"Applying Rescue Mission... this might calculate for a moment...\")\n",
                "df[['SSD_GB', 'HDD_GB']] = df.apply(rescue_storage, axis=1)\n",
                "\n",
                "# Step 3: Finalize (Fill remaining NaNs with 0 if at least one storage exists)\n",
                "df['SSD_GB'].fillna(0, inplace=True)\n",
                "df['HDD_GB'].fillna(0, inplace=True)\n",
                "\n",
                "# Check how many are still totally empty (0 SSD and 0 HDD)\n",
                "empty_storage_count = len(df[(df['SSD_GB'] == 0) & (df['HDD_GB'] == 0)])\n",
                "print(f\"Rows with absolutely NO storage info remaining: {empty_storage_count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Re-Engineering STORAGE_TYPE\n",
                "\n",
                "Since we now have clean numeric values for SSD and HDD, we can create a `STORAGE_TYPE` column that is 100% accurate, replacing the old one which had many missing values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_storage_type(row):\n",
                "    if row['SSD_GB'] > 0 and row['HDD_GB'] > 0:\n",
                "        return 'Hybrid'\n",
                "    elif row['SSD_GB'] > 0:\n",
                "        return 'SSD'\n",
                "    elif row['HDD_GB'] > 0:\n",
                "        return 'HDD'\n",
                "    else:\n",
                "        return 'Unknown'\n",
                "\n",
                "df['STORAGE_TYPE_CLEAN'] = df.apply(get_storage_type, axis=1)\n",
                "\n",
                "# Visualizing the types\n",
                "sns.countplot(x='STORAGE_TYPE_CLEAN', data=df, palette='viridis')\n",
                "plt.title('Distribution of Storage Types After Cleaning')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Dropping the Unsalvageable\n",
                "\n",
                "### Why Drop?\n",
                "Rows with `Unknown` storage type have **0 GB** of storage. A laptop with no storage cannot exist or be priced correctly. Keeping them introduces noise.\n",
                "We drop these ~5,800 rows (approx 10%) to ensure high data quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "initial_len = len(df)\n",
                "df_clean = df[df['STORAGE_TYPE_CLEAN'] != 'Unknown'].copy()\n",
                "final_len = len(df_clean)\n",
                "\n",
                "print(f\"Dropped {initial_len - final_len} rows ({((initial_len - final_len)/initial_len)*100:.2f}%).\")\n",
                "print(f\"Final Clean Dataset Shape: {df_clean.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Export Data\n",
                "Saving the clean data to CSV."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_clean.to_csv(OUTPUT_PATH, index=False)\n",
                "print(f\"File saved successfully to: {OUTPUT_PATH}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}